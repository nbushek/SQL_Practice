{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdeef01",
   "metadata": {},
   "source": [
    "First we import Spark and create a Spark Session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3218bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SuperstoreAssignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705b639",
   "metadata": {},
   "source": [
    "Then we create a dataframe from the sample data set as a .csv file. Then we take a look at the schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aff856df",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Downloads/Superstore.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd0e1a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64afc9",
   "metadata": {},
   "source": [
    "After a little bit of tinkering, a problem with the data set is discovered. One issue is that several of the quantitative variables (e.g., Sales, Quantity, and Discount) are cast as strings. While this might be a problem with inferring the schema and can be manually changed, a little more investigation reveals some entries in the Sales column being strings (e.g., Blue), and the type of numerical values in the same row after the Sales column don't match the expected format. Digging a bit more reveals a problem reading in the Product Name column in the. Several of these descriptions have a single quotation mark (\") to designate inches, which throws off the start and end of the string, which then reads the next comma as the delimeter even though it is supposed to be a part of the string in the Product Name. Converting the Sales to double turns these character entries to NULL, and we can count the number of such instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "34b485af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n",
      "+--------+\n",
      "|Num_Null|\n",
      "+--------+\n",
      "|     300|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Sales\", df.Sales.cast(\"double\"))\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"table_init\")\n",
    "\n",
    "spark.sql(\"SELECT COUNT(`Product ID`) as Num_Null \\\n",
    "            FROM table_init \\\n",
    "            WHERE Sales IS NULL;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11801c0",
   "metadata": {},
   "source": [
    "There are 300 such fields. This is nontrivial, so we'll need to do a find and replace to get rid of all \" occurances within the strings. This could be done in python, but will actually be a lot easier in a spreadsheet. I've done this and created a new .csv file called Superstore1.csv. Now when we load this, Spark correctly infers the variable type of the last four variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "34f08ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"Downloads/Superstore1.csv\"\n",
    "\n",
    "df1 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "\n",
    "df1.printSchema()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185126c",
   "metadata": {},
   "source": [
    "To have a more consistent naming convention and to avoid using backticks on field names with spaces and hyphens, I'm going to rename fields with an underscore convention. \n",
    "\n",
    "Then, we create a SQL table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ab59497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row_ID: integer (nullable = true)\n",
      " |-- Order_ID: string (nullable = true)\n",
      " |-- Order_Date: string (nullable = true)\n",
      " |-- Ship_Date: string (nullable = true)\n",
      " |-- Ship_Mode: string (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal_Code: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub_Category: string (nullable = true)\n",
      " |-- Product_Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumnsRenamed({\"Row ID\": \"Row_ID\", \"Order ID\": \"Order_ID\", \"Order Date\": \"Order_Date\", \\\n",
    "                       \"Ship Date\": \"Ship_Date\", \"Ship Mode\": \"Ship_Mode\", \"Customer ID\": \"Customer_ID\", \\\n",
    "                       \"Customer Name\": \"Customer_Name\", \"Postal Code\": \"Postal_Code\", \\\n",
    "                       \"Product ID\": \"Product_ID\", \"Sub-Category\": \"Sub_Category\",\"Product Name\": \"Product_Name\"})\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1bc1459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c8f99",
   "metadata": {},
   "source": [
    "Since we have several tasks involving revenue and sales, and since we don't have the ability to ask about how to interpret these fields, we'll look at some of these values below. \n",
    "\n",
    "By looking at these (very anecdotally, but sufficient to understand the convention, here) we see that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{Sales} = \\mbox{(Per Item Price)} \\times \\mbox{(1-Discount)} \\times \\mbox{(Quantity)}.\n",
    "\\end{equation}\n",
    "\n",
    "The Per Item Price is not given to us in this data set, but can be reverse engineered using the above relationship. \n",
    "\n",
    "It is most relevant to note that the Sales field is the total Revenue for the particular Order of that row. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f651b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+--------+--------+--------+\n",
      "|     Product_ID|   Sales|Quantity|Discount|  Profit|\n",
      "+---------------+--------+--------+--------+--------+\n",
      "|TEC-PH-10004977|1097.544|       7|     0.2|123.4737|\n",
      "|TEC-PH-10004977| 470.376|       3|     0.2| 52.9173|\n",
      "|TEC-PH-10004977| 627.168|       4|     0.2| 70.5564|\n",
      "|TEC-PH-10004977|  391.98|       2|       0|113.6742|\n",
      "|TEC-PH-10004977| 627.168|       4|     0.2| 70.5564|\n",
      "|TEC-PH-10004977|  391.98|       2|       0|113.6742|\n",
      "|TEC-PH-10004977| 470.376|       3|     0.2| 52.9173|\n",
      "|TEC-PH-10004977|  979.95|       5|       0|284.1855|\n",
      "|TEC-PH-10004977| 235.188|       2|     0.4|-43.1178|\n",
      "|TEC-PH-10004959|  100.49|       1|       0| 25.1225|\n",
      "+---------------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Product_ID, Sales, Quantity, Discount, Profit \\\n",
    "            FROM table \\\n",
    "            ORDER BY Product_ID DESC \\\n",
    "            LIMIT 10;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01017ff2",
   "metadata": {},
   "source": [
    "It is also important to note that each Order may consist of several rows, i.e., the same Order_ID may occur on many rows, one for each Product_ID of that order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aa8edaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------+--------+--------+--------+\n",
      "|      Order_ID|     Product_ID|  Sales|Quantity|Discount|  Profit|\n",
      "+--------------+---------------+-------+--------+--------+--------+\n",
      "|CA-2014-100006|TEC-PH-10002075| 377.97|       3|     0.0|109.6113|\n",
      "|CA-2014-100090|FUR-TA-10003715|502.488|       3|     0.2|-87.9354|\n",
      "|CA-2014-100090|OFF-BI-10001597|196.704|       6|     0.2| 68.8464|\n",
      "|CA-2014-100293|OFF-PA-10000176| 91.056|       6|     0.2| 31.8696|\n",
      "|CA-2014-100328|OFF-BI-10000343|  3.928|       1|     0.2|  1.3257|\n",
      "|CA-2014-100363|OFF-FA-10000611|  2.368|       2|     0.2|  0.8288|\n",
      "|CA-2014-100363|OFF-PA-10004733| 19.008|       3|     0.2|  6.8904|\n",
      "|CA-2014-100391|OFF-PA-10001471|  14.62|       2|     0.0|  6.7252|\n",
      "|CA-2014-100678|OFF-AR-10001868|  2.688|       2|     0.2|   1.008|\n",
      "|CA-2014-100678|FUR-CH-10002602|317.058|       3|     0.3|-18.1176|\n",
      "|CA-2014-100678|OFF-EN-10000056|149.352|       3|     0.2| 50.4063|\n",
      "|CA-2014-100678|TEC-AC-10000474|227.976|       3|     0.2|  28.497|\n",
      "|CA-2014-100706|TEC-AC-10001314|  99.98|       2|     0.0|  7.9984|\n",
      "|CA-2014-100706|FUR-FU-10002268|  29.46|       6|     0.0|  9.7218|\n",
      "|CA-2014-100762|OFF-AR-10000380| 151.92|       4|     0.0|  45.576|\n",
      "|CA-2014-100762|OFF-LA-10003930| 196.62|       2|     0.0| 96.3438|\n",
      "|CA-2014-100762|OFF-PA-10001815| 144.12|       3|     0.0| 69.1776|\n",
      "|CA-2014-100762|OFF-PA-10004082|  15.96|       2|     0.0|    7.98|\n",
      "|CA-2014-100860|OFF-LA-10001982|  18.75|       5|     0.0|     9.0|\n",
      "|CA-2014-100867|TEC-PH-10004922|321.552|       6|     0.2|  20.097|\n",
      "+--------------+---------------+-------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Order_ID, Product_ID, Sales, Quantity, Discount, Profit  \\\n",
    "            FROM table \\\n",
    "            ORDER BY Order_ID;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c4f35",
   "metadata": {},
   "source": [
    "Now, we address the assignment questions. \n",
    "\n",
    "1. Determine the best-selling product sub-category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d81c7edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|Sub_Category|Number_Sold|\n",
      "+------------+-----------+\n",
      "|     Binders|       5974|\n",
      "|       Paper|       5178|\n",
      "| Furnishings|       3563|\n",
      "|      Phones|       3289|\n",
      "|     Storage|       3158|\n",
      "|         Art|       3000|\n",
      "| Accessories|       2976|\n",
      "|      Chairs|       2356|\n",
      "|  Appliances|       1729|\n",
      "|      Labels|       1400|\n",
      "|      Tables|       1241|\n",
      "|   Fasteners|        914|\n",
      "|   Envelopes|        906|\n",
      "|   Bookcases|        868|\n",
      "|    Supplies|        647|\n",
      "|    Machines|        440|\n",
      "|     Copiers|        234|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Sub_Category, SUM(Quantity) as Number_Sold \\\n",
    "            FROM table \\\n",
    "            GROUP BY Sub_Category \\\n",
    "            ORDER BY Number_Sold DESC;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1583dfa",
   "metadata": {},
   "source": [
    "2. Identify the product category generating the highest revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0212793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|       Category|    Total_Revenue|\n",
      "+---------------+-----------------+\n",
      "|     Technology|836154.0329999966|\n",
      "|      Furniture|741999.7952999998|\n",
      "|Office Supplies|719047.0320000029|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Category, SUM(Sales) as Total_Revenue \\\n",
    "             FROM table \\\n",
    "             GROUP BY Category \\\n",
    "             ORDER BY Total_Revenue DESC;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7bbf0b",
   "metadata": {},
   "source": [
    "3. Compile a top 10 list of the most valuable customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4cd925b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|       Customer_Name|      Total_Profit|\n",
      "+--------------------+------------------+\n",
      "|        Tamara Chand| 8981.323900000001|\n",
      "|        Raymond Buch|         6976.0959|\n",
      "|        Sanjit Chand| 5757.411899999999|\n",
      "|        Hunter Lopez|5622.4292000000005|\n",
      "|       Adrian Barton|         5444.8055|\n",
      "|        Tom Ashbrook| 4703.788299999999|\n",
      "|Christopher Martinez|3899.8903999999998|\n",
      "|       Keith Dawkins|         3038.6254|\n",
      "|         Andy Reiter|2884.6207999999997|\n",
      "|       Daniel Raglin|2869.0760000000005|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Customer_Name, SUM(Profit) as Total_Profit \\\n",
    "            FROM table \\\n",
    "            GROUP BY Customer_Name \\\n",
    "            ORDER BY Total_Profit DESC \\\n",
    "            LIMIT 10;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94406ec9",
   "metadata": {},
   "source": [
    "4. Determine the state responsible for the highest number of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6ae3fc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|         State|Number_Orders|\n",
      "+--------------+-------------+\n",
      "|    California|         1021|\n",
      "|      New York|          562|\n",
      "|         Texas|          487|\n",
      "|  Pennsylvania|          288|\n",
      "|      Illinois|          276|\n",
      "|    Washington|          256|\n",
      "|          Ohio|          236|\n",
      "|       Florida|          200|\n",
      "|North Carolina|          136|\n",
      "|      Michigan|          117|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT State, COUNT(DISTINCT Order_ID) as Number_Orders \\\n",
    "            FROM table \\\n",
    "            GROUP BY State \\\n",
    "            ORDER BY Number_Orders DESC \\\n",
    "            LIMIT 10;\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3c5c7",
   "metadata": {},
   "source": [
    "5. Find the year with the highest revenue generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aeb34932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|    Annual_Revenue|\n",
      "+----+------------------+\n",
      "|2017| 733215.2551999999|\n",
      "|2016| 609205.5980000008|\n",
      "|2014| 484247.4981000009|\n",
      "|2015|470532.50899999985|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT years.Year AS Year, SUM(table.Sales) AS Annual_Revenue \\\n",
    "          FROM (SELECT Row_ID AS ID, RIGHT(Order_Date, 4) AS Year \\\n",
    "              FROM table) AS years \\\n",
    "          LEFT JOIN table \\\n",
    "          ON table.Row_ID = years.ID \\\n",
    "          GROUP BY years.Year \\\n",
    "          ORDER BY Annual_Revenue DESCcontinue;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3840becb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
